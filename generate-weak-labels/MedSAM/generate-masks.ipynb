{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e29e62-e337-47e7-8a25-caccefbf3555",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import torch\n",
    "from segment_anything import sam_model_registry\n",
    "from skimage import io, transform\n",
    "import torch.nn.functional as F\n",
    "import cv2\n",
    "from scipy import ndimage\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "import statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d59f3a-cd07-4579-82fa-09c5cd6127fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualization functions\n",
    "# source: https://github.com/facebookresearch/segment-anything/blob/main/notebooks/predictor_example.ipynb\n",
    "# change color to avoid red and green\n",
    "def show_mask(mask, ax, random_color=False):\n",
    "    if random_color:\n",
    "        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
    "    else:\n",
    "        #color = np.array([251/255, 252/255, 30/255, 0.8])\n",
    "        color = np.array([255, 255, 255, 0.8])\n",
    "    h, w = mask.shape[-2:]\n",
    "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
    "    ax.imshow(mask_image)\n",
    "\n",
    "def show_box(box, ax):\n",
    "    x0, y0 = box[0], box[1]\n",
    "    w, h = box[2] - box[0], box[3] - box[1]\n",
    "    ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor='blue', facecolor=(0,0,0,0), lw=2))\n",
    "\n",
    "def show_points(coords, ax, marker_size=100):\n",
    "    pos_points = coords\n",
    "    ax.scatter(pos_points[:, 0], pos_points[:, 1], color='green', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)\n",
    "\n",
    "@torch.no_grad()\n",
    "def medsam_inference(medsam_model, img_embed, box_1024, H, W):\n",
    "    box_torch = torch.as_tensor(box_1024, dtype=torch.float, device=img_embed.device)\n",
    "    if len(box_torch.shape) == 2:\n",
    "        box_torch = box_torch[:, None, :] # (B, 1, 4)\n",
    "\n",
    "    sparse_embeddings, dense_embeddings = medsam_model.prompt_encoder(\n",
    "        points=None,\n",
    "        boxes=box_torch,\n",
    "        masks=None,\n",
    "    )\n",
    "    low_res_logits, _ = medsam_model.mask_decoder(\n",
    "        image_embeddings=img_embed, # (B, 256, 64, 64)\n",
    "        image_pe=medsam_model.prompt_encoder.get_dense_pe(), # (1, 256, 64, 64)\n",
    "        sparse_prompt_embeddings=sparse_embeddings, # (B, 2, 256)\n",
    "        dense_prompt_embeddings=dense_embeddings, # (B, 256, 64, 64)\n",
    "        multimask_output=False,\n",
    "        )\n",
    "\n",
    "    low_res_pred = torch.sigmoid(low_res_logits)  # (1, 1, 256, 256)\n",
    "\n",
    "    low_res_pred = F.interpolate(\n",
    "        low_res_pred,\n",
    "        size=(H, W),\n",
    "        mode=\"bilinear\",\n",
    "        align_corners=False,\n",
    "    )  # (1, 1, gt.shape)\n",
    "    low_res_pred = low_res_pred.squeeze().cpu().numpy()  # (256, 256)\n",
    "    medsam_seg = (low_res_pred > 0.5).astype(np.uint8)\n",
    "    return medsam_seg\n",
    "\n",
    "def medsam_inference_auto(medsam_model, img_embed, box_1024, H, W):\n",
    "    box_torch = torch.as_tensor(box_1024, dtype=torch.float, device=img_embed.device)\n",
    "    if len(box_torch.shape) == 2:\n",
    "        box_torch = box_torch[:, None, :] # (B, 1, 4)\n",
    "\n",
    "    sparse_embeddings, dense_embeddings = medsam_model.prompt_encoder(\n",
    "        points=None,\n",
    "        boxes=None,\n",
    "        masks=None,\n",
    "    )\n",
    "    low_res_logits, _ = medsam_model.mask_decoder(\n",
    "        image_embeddings=img_embed, # (B, 256, 64, 64)\n",
    "        image_pe=medsam_model.prompt_encoder.get_dense_pe(), # (1, 256, 64, 64)\n",
    "        sparse_prompt_embeddings=sparse_embeddings, # (B, 2, 256)\n",
    "        dense_prompt_embeddings=dense_embeddings, # (B, 256, 64, 64)\n",
    "        multimask_output=False,\n",
    "        )\n",
    "\n",
    "    low_res_pred = torch.sigmoid(low_res_logits)  # (1, 1, 256, 256)\n",
    "\n",
    "    low_res_pred = F.interpolate(\n",
    "        low_res_pred,\n",
    "        size=(H, W),\n",
    "        mode=\"bilinear\",\n",
    "        align_corners=False,\n",
    "    )  # (1, 1, gt.shape)\n",
    "    print(low_res_pred)\n",
    "    low_res_pred = low_res_pred.squeeze().cpu().detach().numpy()  # (256, 256)\n",
    "    print(np.max(low_res_pred))\n",
    "    medsam_seg = (low_res_pred > 0.5).astype(np.uint8)\n",
    "    return medsam_seg\n",
    "\n",
    "def medsam_inference_point(medsam_model, img_embed, x_1, y_1, x_2, y_2, H, W):\n",
    "    coords_1024 = np.array([[x_1 * 1024 / W, y_1 * 1024 / H], [x_2 * 1024 / W, y_2 * 1024 / H]])\n",
    "    #\n",
    "    #coords_1024_2 = np.array([[[\n",
    "    #        x_2 * 1024 / W,\n",
    "    #        y_2 * 1024 / H\n",
    "    #    ]]])\n",
    "    coords_torch = torch.tensor([coords_1024], dtype=torch.float32).to(img_embed.device)\n",
    "    labels_torch = torch.tensor([[1, -1]], dtype=torch.long).to(img_embed.device)\n",
    "    point_prompt = (coords_torch, labels_torch)\n",
    "    \n",
    "    sparse_embeddings, dense_embeddings = medsam_model.prompt_encoder(\n",
    "        points=point_prompt,\n",
    "        boxes=None,\n",
    "        masks=None,\n",
    "    )\n",
    "    low_res_logits, _ = medsam_model.mask_decoder(\n",
    "        image_embeddings=img_embed, # (B, 256, 64, 64)\n",
    "        image_pe=medsam_model.prompt_encoder.get_dense_pe(), # (1, 256, 64, 64)\n",
    "        sparse_prompt_embeddings=sparse_embeddings, # (B, 2, 256)\n",
    "        dense_prompt_embeddings=dense_embeddings, # (B, 256, 64, 64)\n",
    "        multimask_output=False,\n",
    "        )\n",
    "\n",
    "    low_res_pred = torch.sigmoid(low_res_logits)  # (1, 1, 256, 256)\n",
    "\n",
    "    low_res_pred = F.interpolate(\n",
    "        low_res_pred,\n",
    "        size=(H, W),\n",
    "        mode=\"bilinear\",\n",
    "        align_corners=False,\n",
    "    )  # (1, 1, gt.shape)\n",
    "    low_res_pred = low_res_pred.detach().cpu().numpy().squeeze()  # (256, 256)\n",
    "    medsam_seg = (low_res_pred > 0.5).astype(np.uint8)\n",
    "    return medsam_seg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4d56e5-06ef-4f17-993a-3b6690e8a737",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load MedSAM model\n",
    "MedSAM_CKPT_PATH = \"medsam_vit_b.pth\"\n",
    "\n",
    "# use the following for point prompts\n",
    "MedSAM_CKPT_PATH = \"medsam_point_prompt_flare22.pth\"\n",
    "\n",
    "device = \"cuda:0\"\n",
    "medsam_model = sam_model_registry['vit_b'](checkpoint=MedSAM_CKPT_PATH)\n",
    "medsam_model = medsam_model.to(device)\n",
    "medsam_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1457d1dd-6085-4135-b9be-3039076a0fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f270a8-eab4-4c4c-8b3c-70bc4f3ca3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_mask_bbox(directory_path, prediction_path, save_path, filename):\n",
    "    \n",
    "    img_np = io.imread(os.path.join(directory_path, filename))\n",
    "    if len(img_np.shape) == 2:\n",
    "        img_3c = np.repeat(img_np[:, :, None], 3, axis=-1)\n",
    "    else:\n",
    "        img_3c = img_np\n",
    "    H, W, _ = img_3c.shape\n",
    "    \n",
    "    img_1024 = transform.resize(img_3c, (1024, 1024), order=3, preserve_range=True, anti_aliasing=True).astype(np.uint8)\n",
    "    img_1024 = (img_1024 - img_1024.min()) / np.clip(\n",
    "        img_1024.max() - img_1024.min(), a_min=1e-8, a_max=None\n",
    "    )  # normalize to [0, 1], (H, W, 3)\n",
    "    # convert the shape to (3, H, W)\n",
    "    img_1024_tensor = torch.tensor(img_1024).float().permute(2, 0, 1).unsqueeze(0).to(device)\n",
    "\n",
    "    \n",
    "    # using the 50-image model prediction, dynamically threshold the map using thresholds of 70%, 65%, or 40%.\n",
    "    image_mask = np.where(cv2.imread(os.path.join(prediction_path, filename.replace('.jpg', '.png')),  cv2.IMREAD_GRAYSCALE) < 120, 0, 1)\n",
    "    if image_mask.sum() == 0:\n",
    "        image_mask = np.where(cv2.imread(os.path.join(prediction_path, filename.replace('.jpg', '.png')),  cv2.IMREAD_GRAYSCALE) < 168, 0, 1)\n",
    "    if image_mask.sum() == 0:\n",
    "        image_mask = np.where(cv2.imread(os.path.join(prediction_path, filename.replace('.jpg', '.png')),  cv2.IMREAD_GRAYSCALE) < 100, 0, 1)\n",
    "    if image_mask.sum() == 0:\n",
    "        image_mask = np.where(cv2.imread(os.path.join(prediction_path, filename.replace('.jpg', '.png')),  cv2.IMREAD_GRAYSCALE) < 50, 0, 1)\n",
    "    if image_mask.sum() == 0:\n",
    "        image_mask = np.where(cv2.imread(os.path.join(prediction_path, filename.replace('.jpg', '.png')),  cv2.IMREAD_GRAYSCALE) < 25, 0, 1)\n",
    "\n",
    "    if image_mask.sum() == 0:\n",
    "        print(filename)\n",
    "        return\n",
    "    \n",
    "    # find the bounding box using the min/max row/column in the thresholded map\n",
    "    labeled_array, num_features = ndimage.label(image_mask)\n",
    "    component_sizes = np.bincount(labeled_array.ravel())\n",
    "    largest_component_label = np.argmax(component_sizes[1:]) + 1\n",
    "    indices = np.argwhere(labeled_array == largest_component_label)\n",
    "\n",
    "    indices = np.argwhere(image_mask == 1)\n",
    "    row_indices = indices[:, 0]\n",
    "    col_indices = indices[:, 1]    \n",
    "    bbox = [min(col_indices), min(row_indices), max(col_indices), max(row_indices)]\n",
    "    bbox2 = [0, 0, 0, 0]\n",
    "    bbox2[1] = bbox[1] * (H/image_mask.shape[0])\n",
    "    bbox2[0] = bbox[0] * (W/image_mask.shape[1])\n",
    "    bbox2[3] = bbox[3] * (H/image_mask.shape[0])\n",
    "    bbox2[2] = bbox[2] * (W/image_mask.shape[1])\n",
    "\n",
    "\n",
    "    box_np = np.array(np.round([bbox2]))\n",
    "    # transfer box_np to 1024x1024 scale\n",
    "    box_1024 = box_np / np.array([W, H, W, H]) * 1024\n",
    "    with torch.no_grad():\n",
    "        image_embedding = medsam_model.image_encoder(img_1024_tensor) # (1, 256, 64, 64)\n",
    "    medsam_seg = medsam_inference(medsam_model, image_embedding, box_1024, H, W)\n",
    "\n",
    "    fig, ax = plt.subplots(1, 3, figsize=(10, 5))\n",
    "    ax[0].imshow(img_3c)\n",
    "    ax[0].set_title(\"Input Image and Bounding Box\")\n",
    "    ax[1].imshow(img_3c)\n",
    "    show_mask(medsam_seg, ax[1])\n",
    "    show_box(box_np[0], ax[1])\n",
    "    ax[1].set_title(\"MedSAM Segmentation\")\n",
    "    ax[2].imshow(image_mask)\n",
    "    show_box(np.array(bbox), ax[2])\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    binary_array = medsam_seg.astype(np.uint8) * 255\n",
    "    percent_target = binary_array.sum() / (binary_array.shape[0] * binary_array.shape[1]*255) \n",
    "    output_path = os.path.join(save_path, filename.replace(\"_mask.png\", \".png\").replace('.jpg', '.png'))\n",
    "    plt.imsave(output_path, binary_array, cmap='gray')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928cf3d9-c660-422e-8c65-f7e7a3b118f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_mask_points(directory_path, prediction_path, save_path, filename):\n",
    "    \n",
    "    # read in the original image from the directory path and preprocess according to medSAM requirements\n",
    "    #filename = filename.replace('.jpg', '.png')\n",
    "    img_np = io.imread(os.path.join(directory_path, filename))\n",
    "    if len(img_np.shape) == 2:\n",
    "        img_3c = np.repeat(img_np[:, :, None], 3, axis=-1)\n",
    "    else:\n",
    "        img_3c = img_np\n",
    "    H, W, _ = img_3c.shape\n",
    "    \n",
    "    img_1024 = transform.resize(img_3c, (1024, 1024), order=3, preserve_range=True, anti_aliasing=True).astype(np.uint8)\n",
    "    img_1024 = (img_1024 - img_1024.min()) / np.clip(\n",
    "        img_1024.max() - img_1024.min(), a_min=1e-8, a_max=None\n",
    "    )  # normalize to [0, 1], (H, W, 3)\n",
    "    # convert the shape to (3, H, W)\n",
    "    img_1024_tensor = torch.tensor(img_1024).float().permute(2, 0, 1).unsqueeze(0).to(device)\n",
    "\n",
    "    \n",
    "    # using the 50-image model prediction, dynamically threshold the map.\n",
    "    image_mask = np.where(cv2.imread(os.path.join(prediction_path, filename.replace('.jpg', '.png')),  cv2.IMREAD_GRAYSCALE) < 225, 0, 1)\n",
    "    if image_mask.sum() == 0:\n",
    "        image_mask = np.where(cv2.imread(os.path.join(prediction_path, filename.replace('.jpg', '.png')),  cv2.IMREAD_GRAYSCALE) < 168, 0, 1)\n",
    "    if image_mask.sum() == 0:\n",
    "        image_mask = np.where(cv2.imread(os.path.join(prediction_path, filename.replace('.jpg', '.png')),  cv2.IMREAD_GRAYSCALE) < 100, 0, 1)\n",
    "    if image_mask.sum() == 0:\n",
    "        image_mask = np.where(cv2.imread(os.path.join(prediction_path, filename.replace('.jpg', '.png')),  cv2.IMREAD_GRAYSCALE) < 50, 0, 1)\n",
    "    if image_mask.sum() == 0:\n",
    "        image_mask = np.where(cv2.imread(os.path.join(prediction_path, filename.replace('.jpg', '.png')),  cv2.IMREAD_GRAYSCALE) < 25, 0, 1)\n",
    "\n",
    "    if image_mask.sum() == 0:\n",
    "        return\n",
    "    \n",
    "    false_indices = np.argwhere(~image_mask)\n",
    "    random_false_index = list(random.choice(false_indices))\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    labeled_array, num_features = ndimage.label(image_mask)\n",
    "    component_sizes = np.bincount(labeled_array.ravel())\n",
    "    largest_component_label = np.argmax(component_sizes[1:]) + 1\n",
    "    \n",
    "    indices = np.argwhere(labeled_array == largest_component_label)\n",
    "    row_indices = indices[:, 0]\n",
    "    col_indices = indices[:, 1] \n",
    "    \n",
    "    # uncomment if using second-largest component as well\n",
    "    #second_largest_label = np.argsort(component_sizes[1:])[-2] + 1\n",
    "    #indices2 = np.argwhere(labeled_array == second_largest_label)\n",
    "    #row_indices2 = indices2[:, 0]\n",
    "    #col_indices2 = indices2[:, 1] \n",
    "    #point3 = [statistics.median(col_indices2), statistics.median(row_indices2)]\n",
    "    #point4 = [0, 0]\n",
    "    #point4[1] = point3[1] * (H/image_mask.shape[0])\n",
    "    #point4[0] = point3[0] * (W/image_mask.shape[1])\n",
    "    \n",
    "    point = [statistics.median(col_indices), statistics.median(row_indices)]\n",
    "    point2 = [0, 0]\n",
    "    point2[1] = point[1] * (H/image_mask.shape[0])\n",
    "    point2[0] = point[0] * (W/image_mask.shape[1])\n",
    "\n",
    "    false_point = [0, 0]\n",
    "    false_point[1] = random_false_index[1] * (H/image_mask.shape[0])\n",
    "    false_point[0] = random_false_index[0] * (W/image_mask.shape[1])\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    box_np = np.array(np.round([point2]))\n",
    "    box_np_2 = np.array(np.round([false_point]))\n",
    "    #box_np_3 = np.array(np.round([point4]))\n",
    "\n",
    "\n",
    "    # transfer box_np t0 1024x1024 scale\n",
    "    box_1024 = box_np / np.array([W, H]) * 1024\n",
    "    #box_1024_2 = box_np_3 / np.array([W, H]) * 1024\n",
    "\n",
    "    with torch.no_grad():\n",
    "        image_embedding = medsam_model.image_encoder(img_1024_tensor) # (1, 256, 64, 64)\n",
    "    medsam_seg = medsam_inference_point(medsam_model, image_embedding, box_np[0][0], box_np[0][1], box_np_2[0][0], box_np_2[0][1], H, W)\n",
    "\n",
    "    fig, ax = plt.subplots(1, 3, figsize=(10, 5))\n",
    "    ax[0].imshow(img_1024)\n",
    "    show_points(box_1024, ax[0])\n",
    "    #s#how_points(box_1024_2, ax[0])\n",
    "\n",
    "    ax[0].set_title(\"Input Image and Bounding Box\")\n",
    "    ax[1].imshow(img_3c)\n",
    "    show_mask(medsam_seg, ax[1])\n",
    "    show_points(box_np, ax[1])\n",
    "    ax[1].set_title(\"MedSAM Segmentation\")\n",
    "    ax[2].imshow(image_mask)\n",
    "    show_points(np.array([point]), ax[2])\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    binary_array = medsam_seg.astype(np.uint8) * 255\n",
    "    output_path = os.path.join(save_path, filename.replace(\"_mask.png\", \".png\").replace('.jpg', '.png'))\n",
    "    plt.imsave(output_path, binary_array, cmap='gray')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3695065-d735-45cc-a4f7-a091791120c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"../pytorch-nested-unet\"\n",
    "\n",
    "# aug_path: path of the unlabeled dataset\n",
    "aug_path = \"busi-box-aug-50\"\n",
    "# base_dataset: path of the gold-standard dataset\n",
    "base_dataset = \"busi-25-small\"\n",
    "\n",
    "image_path = os.path.join(base_path, \"inputs\", aug_path, \"images\")\n",
    "save_path = os.path.join(base_path, \"inputs\", aug_path, \"masks/0\")\n",
    "\n",
    "# formatted like ../pytorch-nested-unet/outputs/<model name>/<unlabeled dataset name>/0/\"\n",
    "prediction_path = \"../pytorch-nested-unet/outputs/busi-25-small_NestedUNet_256_woDS/busi-box-aug-50/0/\"\n",
    "\n",
    "masks_to_gen = os.listdir(os.path.join(base_path, \"inputs\", aug_path, \"images\"))\n",
    "\n",
    "for filename in tqdm(masks_to_gen):\n",
    "    if os.path.isfile(os.path.join(directory_path, filename)):\n",
    "        # replace with make_mask_points if using point prompts\n",
    "        make_mask_bbox(directory_path, prediction_path, save_path, filename)\n",
    "        \n",
    "# copy over base dataset to form augmented dataset\n",
    "base_images = os.listdir(os.path.join(base_path, \"inputs\", base_dataset, \"images\"))\n",
    "base_masks = os.listdir(os.path.join(base_path, \"inputs\", base_dataset, \"masks/0\"))\n",
    "\n",
    "for filename in base_images:\n",
    "    shutil.copyfile(os.path.join(base_path, \"inputs\", base_dataset, \"images\", filename), os.path.join(image_path, filename))\n",
    "\n",
    "for filename in base_masks:\n",
    "    shutil.copyfile(os.path.join(base_path, \"inputs\", base_dataset, \"masks/0\", filename), os.path.join(save_path, filename))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medsam",
   "language": "python",
   "name": "medsam"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
